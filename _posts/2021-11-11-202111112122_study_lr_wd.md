---
layout: post
title: "About lr, wd"
category: study
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---



## 실험하면서,,,

train loss가 거의 안 떨어지면 → lr이 너무 크거나, lr이 너무 작거나, wd가 너무 큰 것.

loss가 조금씩 → lr이 너무 작아.

<br>

train이랑 val 차이가 너무 크잖아. → overfitting → wd를 키워줘. 

train loss가 안 낮아져 → wd가 커서 그래. 

<br>

wd는 l2 norm penalty로 주는 거. 너무 크면 update가 안돼.

dropout이 너무 커서 train loss가 안 떨어질 수 있다.

