---
layout: post
title: "About scheduler"
category: study
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---



~~매일 잊어먹어서.. 한 번쯤 정리~~

## scheduler

- StepLR

  - Parameter

    - step_size : 몇 epoch 마다 lr을 감소
    - gamma : gamma 비율로 lr을 감소

  - step_size마다 gamma 비율로 lr을 감소시킴

  - 즉, step_size마다 gamma를 곱한다.

  - ```python
    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
    ```

    

- MultiStepLR

  - Parameter
    - milestones : lr 줄일 epoch index의 list
    - gamma : gamma 비율로 lr 감소
  - lr을 감소시킬 epoch 지정



- ExponentialLR
  - Parameter
    - gamma : gamma 비율로 lr 감소
  - learning rate decay가 exponential 함수



- CosineAnnealingLR
  - Prameter
    - T_max : 최대 iteration 횟수
    - eta_min : 최소로 떨어질 수 있는 learning rate default=0
  - lr이 cosine 함수를 따라서 eta_min까지 떨어졌다가 다시 초기 learning rate까지 올라간다.
