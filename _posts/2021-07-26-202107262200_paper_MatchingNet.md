---
layout: post
title: "Matching networks for one shot learning"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---

O. Vinyals, C. Blundell, T. Lillicrap, and D. Wierstra, "Matching networks for one shot learning," NIPS, 2016

*few-shot 기본 메소드 살펴보기, 첫 번째 이야기*

*여전히 모르겠다아*

<br>

## Abstract

- metric learning based
- obviating the need for fine-tuning to adapt to new class types

<br>

<br>

## Introduction

딥러닝 학습이 오래 걸리는 이유 → parametric aspect → non-parametric aspect를 이용하자

**새로운 데이터 point를 빠르게 학습하면서도 기존의 학습 내용이 극심하게 사라지지 않는다!!!**

<br>

### Contributions

- 거리 기반의 방식으로 모델 architecture 제시
- 대부분 메타러닝에서 학습 방법으로 많이 쓰는 메타러닝에 특화된 episodic training 제시
- 대량의 큰 dataset인 ImageNet을 가공해서 mini-ImageNet data 제공

<br>

<br>

## Model Architecture

![image](https://user-images.githubusercontent.com/85778937/128876625-66e51d55-25c8-4ff8-9592-219ec7d2ded5.png)

f, g 설정은..

- f, g를 동일하게 설정 가능. 각각 별도로 진행해도 okay
- scratch 학습 (conv 3~4개)
- pretrained 이용 (VGG, Inception 등) 해도 무방

<br>

The key point is that when trained, Matching Networks are able to produce sensible test labels for unobserved classes **without any changes to the network.**

<br>

![image](https://user-images.githubusercontent.com/85778937/128877444-ae5eeaad-0ab6-4bef-8e02-f444695eee99.png)

<br>

### The Attention Kernel

cosine similarity를 통해 유사도 계산 

→ softmax를 통해서 해당하는 kernel density estimator를 구하게 된다

<br>

### Full Context Embeddings

~~근데 여기 사실 잘 모르겠다..~~

복잡한 task인 경우, 고도화된 모델 필요하다고 생각해서 제안

이전에는 support set 관계 자체에서는 독립이라고 보았다. → support set 자체에서도 연관성을 부여하고자 (dependent를 부여하고자) conv 3~4개를 feature로 뽑고, BiLSTM을 통해서 서로 간의 연관 정보를 반영해주는 feature를 뽑기

<br>

- support set 관점
  - independently embedding 막기 → Bi-LSTM으로 추가 embedding
- batch set 관점
  - dependent embedding 된 support set 정보 받기
  - batch set에 attLSTM으로 추가 embedding

<br>

<br>

## Training Strategy

일반적으로 batch training (simple strategy) → Limited data에서 일반적인 지도학습으로 학습이 잘 안됨.

Episode training → overfitting 방지

![image](https://user-images.githubusercontent.com/85778937/128878826-48203226-cd0b-462e-ad1e-9bdb5ad289a1.png)

