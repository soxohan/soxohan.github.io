---
layout: post
title: "Prototypical cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---

X. Yue, Z. Zheng, S. Zhang, Y. Gao, T. Darrell, K. Keutzer, A. S. Vincentelli, "Prototypical cross-domain Self-supervised Learning for Few-shot Unsupervised Domain Adaptation," *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2021, pp. 13834-13844

----

## Abstract

label을 collect하는 건 비용이 많이 든다. 이러한 문제를 해결하기 위해, 최근에 instance-wise cross-domain self-supervised learning을 많이 사용한다.  → However, only learns and aligns low-level discriminative features!

**propose an end-to-end Prototypical Cross-domain Self-Supervised Learning (PCS) framework for Few-shot Unsupervised Domain Adaptation (FUDA)**

PCS not only performs <u>cross-domain low-level feature alignment</u>, but it also  <u>encodes and aligns semantic structures in the shared embedding space across domains</u>.

<br>

<br>

## Introduction

> Despite high accuracy, deep neural network trained on specific datasets often fail to generalize to new domains owing to the domain shift problem.

labeling cost 때문에, few-shot unsupervised domain adaptation (FUDA) 세팅을 고려!

(source 데이터의 일부분만 라벨링이 되어있고, 남은 source 데이터와 target 데이터는 labelling이 되지 않은 상태)

<br>

**PCS performs...**

1. indomain prototypical self-supervision to implicitly encode the semantic structure of data into the embedding space
2. cross-domain instance-to-prototype matching to transfer knowledge from source to the target in a more robust manner.
3. unifies prototype learning with cosine classifier and update consine classifier adaptively with source and target prototypes.

<br>

### Contributions

- propose a novel Prototypical Cross-domain Self-supervised learning framework (PCS) for few-shot unsupervised Domain Adaptation
- propose to leverage prototypes to perform better semantic structure learning, discriminative feature learning, and cross-domain alignment in a unified, unsupervised and adaptive manner
- While it is hard to choose the optimal domain adaptation method in the complex two-stage framework, <u>PCS can be easily trained in an end-to-end matter</u>, and outperforms all state-of-the-art methods by a large margin across multiple benchmark datasets.

<br>

<br>

## Approach

![image](https://user-images.githubusercontent.com/85778937/125191747-f9d25580-e27e-11eb-9d4a-3ade7e53205f.png)

![image](https://user-images.githubusercontent.com/85778937/125191759-048cea80-e27f-11eb-8d90-45965c0eef38.png)



### 1. In-domain Prototypical Contrastive Learning

contrastive learning을 source data와  target data 각각 진행

이것의 목적은 도메인이 cross 되면서의 부적절한 클러스터링과 무차별적인 feature learning을 방지하기 위함이다.

<br>

### 2. Cross-domain Instance-Prototype SSL

explicitly enforce learning domain-aligned and more discriminative features in both source and target domains를 위해서 수행

perform entropy minimization on the similarity distribution vector between its representation

<br>

### 3. Adaptive Prototypical Classifier Learning

goal

→ to learn a better domain-aligned, discriminative feature encoder F and more importantly, a cosine classifier C that could achieve high accuracy on the target domain

#### Adaptive Prototype-Classifier Update (APCU)

#### Mutual Information Maximization

the network is desired to have enough confident predictions

- to promote the network to ohave diversified outputs over the dataset
  - maximize the entropy of expected network prediction 
- in order to get high-confident prediction for each sample
  - leverage entropy minimization on the network output which has shown efficacy in label-scarce scenarios

<br>

### 4. PCS Learning for FUDA

모든 loss를 합침

<br>

<br>

## Experiments

### Datasets

4  public datasets

- Office
- Office-Home
- VisDA-2017
- Domain-Net
