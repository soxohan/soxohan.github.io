---
layout: post
title: "Public dataset_version1"
category: my_progress
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false

---

~~오늘부터 여기에 기록하면서 할거다!!!! 약간 일기장 너낌으로😏~~

🗓️ 2022.12.19

## Public dataset and benchmarks for anomaly detection in time series

[정리가 잘되어있는 깃허브!](https://github.com/zamanzadeh/ts-anomaly-benchmark)

이 부분에서 해야할 것!

- 어떤 데이터셋을 쓸 것인지 확정하자.
- 쓰려고 하는 데이터셋의 형태를 테이블 및 시각적으로 표현하자.
- 다시 돌아오면 안되는 부분이므로 확실히 합시당.

🗓️ 2022.12.21 ~ 2022.12.23

우선 데이터로더를 만들기 전에 데이터에 대한 분석이 필요...?

<img width="675" alt="image" src="https://user-images.githubusercontent.com/85778937/208831756-0f0547b9-04de-4448-86a8-ea7fff1698f6.png">

주로 사용하는 것은 하이라이트 친 것.

### MSL & SMAP

이 데이터는 Mars Science Laboratory dataset으로 SMAP 데이터셋과 유사하다. 

두 개의 데이터 모두 나사에서 제공되는 화성 탐사와 관련된 데이터.

> corresponds to the sensor and actuator data for the MArs rover itself.
>
> this dataset is known to have many trivial sequences
>
> [*TranAD*]
>
> The dataset consists of telemetry data from the NASA Soil Moisture Active Passive (SMAP) satellite as well as the NASA Mars Science Laboratory (MSL) rover, Curiosity. It contains 54 and 27 channels of data for SMAP and MSL, respectively. This data has also already been split into training and testing sets. We further split the training data for each channel in a ratio of 4 : 1 to obtain our training and validation data, respectively. We normalize this data to have values between 0 and 1 as well. Let us take this SMAP/MSL dataset with u = 81 channels as Y.
>
> 
>
> The anomaly labels and metadata are available in `labeled_anomalies.csv`, which includes:
>
> - `channel id`: anonymized channel id - first letter represents nature of channel (P = power, R = radiation, etc.)
> - `spacecraft`: spacecraft that generated telemetry stream
> - `anomaly_sequences`: start and end indices of true anomalies in stream
> - `class`: the class of anomaly (see paper for discussion)
> - `num values`: number of telemetry values in each stream
>
> To provide your own labels, use the `labeled_anomalies.csv` file as a template. The only required fields/columns are `channel_id` and `anomaly_sequences`. `anomaly_sequences` is a list of lists that contain start and end indices of anomalous regions in the test dataset for a channel.

TranAD에서는 3 non-trivial (A4, C2, and T1)의 데이터만 고려.

SMAP(Soil Moisture Active Passive Satellite) : 54개의 데이터

MSL(Mars Science Laboratory rover) : 27개의 데이터

train data의 20%를 validation으로 사용하였음. (논문에서는)

[모야 kaggle에 좋은 로더가 있넹](https://www.kaggle.com/code/patrickfleith/smap-msl-let-s-get-started)

[detail한 설명 github!](https://github.com/khundman/telemanom)

```
wget https://s3-us-west-2.amazonaws.com/telemanom/data.zip && unzip data.zip && rm data.zip

cd data && wget https://raw.githubusercontent.com/khundman/telemanom/master/labeled_anomalies.csv
```

[*Detecting spacecraft anomalies using using LSTMs and non-parametric dynamic thresholding*]

<br>

**까보쟈규!!**

### MSL

해당하는 열은 다음과 같다.

> ['M-6', 'M-1', 'M-2', 'S-2', 'P-10', 'T-4', 'T-5', 'F-7', 'M-3', 'M-4', 'M-5', 'P-15', 'C-1', 'C-2', 'T-12', 'T-13', 'F-4', 'F-5', 'D-14', 'T-9', 'P-14', 'T-8', 'P-11', 'D-15', 'D-16', 'M-7', 'F-8']

각각의 shape도 찍어보면!

<img width="82" alt="image" src="https://user-images.githubusercontent.com/85778937/208845236-b20cdcb3-850a-4ad8-8a01-d3d26c0a20fd.png">

총 27개의 데이터.

55개의 열을 가진 데이터들!

이거는 C-2 데이터의 첫 컬럼!

![image](https://user-images.githubusercontent.com/85778937/209274192-5940a096-7439-4af5-9f4c-72ea416403d2.png)



<br>

### SMAP

해당하는 열은 다음과 같다.

> ['P-1', 'S-1', 'E-1', 'E-2', 'E-3', 'E-4', 'E-5', 'E-6', 'E-7', 'E-8', 'E-9', 'E-10', 'E-11', 'E-12', 'E-13', 'A-1', 'D-1', 'P-2', 'P-3', 'D-2', 'D-3', 'D-4', 'A-2', 'A-3', 'A-4', 'G-1', 'G-2', 'D-5', 'D-6', 'D-7', 'F-1', 'P-4', 'G-3', 'T-1', 'T-2', 'D-8', 'D-9', 'F-2', 'G-4', 'T-3', 'D-11', 'D-12', 'B-1', 'G-6', 'G-7', 'P-7', 'R-1', 'A-5', 'A-6', 'A-7', 'D-13', 'P-2', 'A-8', 'A-9', 'F-3']

총 55개의 데이터. 25개의 feature를 가지고 있다.

데이터는 대략 이런 다음과 같이 생겼다. ~~근데 왜 test 데이터가 train 데이터의 4배나 되는 거죠..?~~

<img width="157" alt="image" src="https://user-images.githubusercontent.com/85778937/209085246-b273ddd5-bf74-435e-b7cd-59e521ecafdf.png">

SMAP에는 A-4랑 T-1 데이터가 있다.

우선 뭐.. A-4의 데이터를 한 번 찍어보자. 라벨과 함께.. 

라벨을 살펴보니 4550번째 구간부터 4659번째 구간까지(110개!)가 1의 값을 가지고 있다. point anomaly가 아니네..

이런건 이쁘게 찍어봐야 잘 보여. 첫번째 컬럼을 오려온 것인데, 

![image](https://user-images.githubusercontent.com/85778937/209259779-8ccd3604-92dd-4068-bb25-34b0bf658c42.png)

이번에는 T-1 데이터의 첫 컬럼!

![image](https://user-images.githubusercontent.com/85778937/209274315-ce7b6de7-fd11-4885-bdc8-4bd2b2075cc2.png)

<br>

### SWaT

Singapore University of Technology and Design의 기관인 iTrust에서 공개한 데이터셋

CPS(Cyber Physical System)

여과수를 생산하는 실제 산업용 공장에서의 11일간의 기록.

- 7일간의 정상 동작에서 수집한 데이터
- 이상 동작을 포함하는 4일간의 데이터

모니터링 센서들의 array로 구성

- Level Indication Transmitter (measured in mm)
- Flow Indication Transmitter (m^3/hr)
- Analyser Indicator Transmitter
  - Conductivity (\muS/cm)
  - pH
  - Oxidation Reduction Potential (mV)
- Differential Pressure Indicator Transmitter (kPa)
- Pressure Indicator Transmitter (kPa)

데이터를 다운로드하기 위해서는 [여기](https://itrust.sutd.edu.sg/itrust-labs_datasets/)에서 form을 작성해야 함! (2022.12.22 17:00 작성 완료 및 제출 완료!) (오잉. 엄청 빨리 왔다. 16:55에 데이터 링크 도착!!)

아래는 기억해야하는 form 내용 일부..

<img width="632" alt="image" src="https://user-images.githubusercontent.com/85778937/209083528-7f9ccba2-0fb8-47c7-9b75-40d525719063.png">

<img width="621" alt="image" src="https://user-images.githubusercontent.com/85778937/209083783-5806fcb5-28be-4507-bc99-f463795d0c77.png">

~~아니 근데 데이터가 무지 많아.. 뿌엥😞~~

<img width="223" alt="image" src="https://user-images.githubusercontent.com/85778937/209088401-51a79fef-c0be-4026-ad2f-4de593fa24b0.png">

[우와 이런것도 있어!](https://github.com/cbhua/tool-swat-preprocess)

~~데이터를..하나하나 다 까보고, shape맞는걸 찾아야겠지..? 내가 못찾는거겠지 ㅜㅜㅜㅜ~~

| 폴더이름              | 파일이름                    | shape          |
| --------------------- | --------------------------- | -------------- |
| SWaT.A8_June 2021.zip | 여러 파일들..               | 컬럼은 모두 95 |
| SWaT.A7_June 2020     | 22June2020 (1).xlsx         | (14400, 82)    |
|                       | 22June2020 (2).xlsx         | (3600, 82)     |
|                       | 29June 2020(1).xlsx         | (7201, 16382)  |
|                       | 29June 2020(2).xlsx         | (7201, 61)     |
| SWaT.A6_Dec 2019      | 왜 gsheet야!!               | 80개정도       |
| SWaT.A4 & A5_Jul 2019 | SWaT_dataset_Jul 19 v2.xlsx | (14998, 78)    |
|                       | SWaT_dataset_Jul 19.xlsx    | (14998, 78)    |

~~아니 대체 어떤 데이터를 쓴거야??????????🤬~~

51행짜리 데이터........ 아니면 전처리를 했나?

<br>

### WADI

Water Distribution(WADI) [여기서 내용을 확인할 수 있다!](https://itrust.sutd.edu.sg/testbeds/water-distribution-wadi/)

SWaT의 확장으로 물 분배 데이터셋을 의미. 16일간의 기록.

- 2 elevated reservoir tanks
- 6 consumer tanks
- 2 raw water tanks
- returned tank
- It also with chemical dosing systems, booster pumps and valves, instrumentation and analysers.

> Together with SWaT, WADI provides opportunities for researchers to work on a full spectrum of possible cyber and physical attacks on a water treatment and distribution plant.

<img width="678" alt="image" src="https://user-images.githubusercontent.com/85778937/209072507-a1bd61da-03c8-4029-9457-dae05f6aec80.png">

WADI all 데이터에서 보면... 2개의 폴더가 존재함.

<img width="248" alt="image" src="https://user-images.githubusercontent.com/85778937/209286469-57814617-d632-41a1-922d-39e537a8d87f.png">

WAdI.A1_9 Oct 2017의 폴더안에 있는 csv가 열리지 않았다. 알고보니 첫 4행이 데이터가 아니여서.. skiprows로 처리하였다.

그런데.. 나서 데이터들의 shape을 찍어보니, 열이 130개였다. 열의 개수가 안맞아ㅏ🥲

TranAD에서는 WADI dimension이 123개고, anomaly detection survey(위 표)에서는 127개였단 말이다..

row수도 세보니까 하나도 안맞다. 우선 논문상 으로볼 땐, WADI 데이터를 제외한 데이터는 개수가 맞던데, 나머지를 먼저 처리하고 와서 이 아이의 사용 유무를 결정해야 할 것 같다.. 😞

<br>

<br>

---

Multivariate data에 보기 좋은 함수를 찾았다!

```python
def multivariate_data(dataset, target, 2start_index, end_index, history_size,
                      target_size, step, single_step=False):
  data = []
  labels = []

  start_index = start_index + history_size
  if end_index is None:
    end_index = len(dataset) - target_size

  for i in range(start_index, end_index):
    indices = range(i-history_size, i, step)
    data.append(dataset[indices])

    if single_step:
      labels.append(target[i+target_size])
    else:
      labels.append(target[i:i+target_size])

  return np.array(data), np.array(labels)
```

~~range함수 저렇게 잘라쓸수있고만..~~

---

### MERLIN

🗓️ 2022.12.23

[MERLIN webpage](https://sites.google.com/view/merlin-find-anomalies/datasets?authuser=0)

여기에서 본인들이 사용한 데이터를 zip 파일 형태로 올려두었다. 

읽어보고 여기있는 데이터들을 사용할지 판단하려 한다.

📖 읽어보니..

MERLIN은 subtle anomaly를 판단하고 싶어서, 어려운 데이터셋인 3가지를 골랐다. ~~좋아 나도 이렇게 할래~~