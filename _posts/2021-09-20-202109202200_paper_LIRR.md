---
layout: post
title: "Learning Invariant Representation and Risks for Semi-supervised Domain Adaptation"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---

B. Li, Y. Wang, S. Zhang, D. Li, K. Keutzer, T. Darrell, and H. Zhao, “Learning Invariant Representations and Risks for Semi-supervised Domain Adaptation,” *CVPR*, 2021, pp. 1104-1113.

~~그림에 속아서 읽게된 논문.. 하하~~

<br>

## Abstract

- Propose the first method that aims to simultaneously learn invariant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA)

<br>

![image](https://user-images.githubusercontent.com/85778937/135720037-a0e6e262-d89c-4a5a-bf21-607408741a0e.png)

<br>

#### Contributions

- Finite-sample generalization bounds for Semi-DA on both classification and regression
- LIRR, to jointly learn invariant representations and invariant optimal predictors
  - In order to mitigate the accuracy discrepancy across domains for better generalization

<br>

![image](https://user-images.githubusercontent.com/85778937/135720126-1e9ea02c-9b03-4dbd-85d0-6e7807759d86.png)

<br>

<br>

## Methods

PAC 분야의 논문인 듯 하다. 

PAC 분야는 이론적으로 모델의 성능을 측정하는 방법.

개념적으로 왜 특정 모델이 좋은지 설명 가능.

<br>

### Generalization bounds for semi-supervised domain adaptation

![image](https://user-images.githubusercontent.com/85778937/135720255-286f1592-5409-465c-8694-b1546697d8a9.png)

수식 너무나 어려운 것..

<br>

### Learning invariant representations and risks

- Information theoretic interpretation
- Algorithm design

<br>

![image](https://user-images.githubusercontent.com/85778937/135720338-5d04ed7b-8150-454d-8be1-3278f5ac8868.png)

이렇게 표현할 수 있는데, 코드로 구현하기 힘들기 때문에, 이 수식을 바꿔나가게 된다.

이 수식을 바꿔나갈 때, 이런 것들을 이용한다.

D(domain index)와 Z(input X로부터 transformation)가 orthogonal을 만족하면 invariance 한건데, 이것은 I(D;Z)를 minimization 시키는 것과 동일하다.

즉, I(D;Z) = 0 이면, D_s(Z) = D_t(Z) 인 것이다.

<br>

최종 수식은 다음과 같다. 

![image](https://user-images.githubusercontent.com/85778937/135720417-c024e164-eff6-465b-a733-8dce51a26204.png)

하나씩 보게 되면, 각각 이렇게 표현된다.

![image](https://user-images.githubusercontent.com/85778937/135720455-5f8fa787-ef0d-4585-a5a2-0e49aa82acca.png)

<br>

여기서, L_d와 L_i는 다음을 의미한다!

![image](https://user-images.githubusercontent.com/85778937/135720471-787d36ee-13c1-44d4-b5ca-38cf4ed9ab14.png)

<br>

<br>

## Results

- Experiment
  - NICO, VisDA2017, OfficeHome, DomainNet datasets
  - S+T : a model trained with the labeled source and the few labeled target samples without using unlabeled target samples
  - Full T : a model trained with the fully labeled target

![image](https://user-images.githubusercontent.com/85778937/135720683-c08b4701-a1b6-446d-8dcd-782c162f7c79.png)

<br>

이 밖에도 여러가지 ablation study를 해보았다.

![image](https://user-images.githubusercontent.com/85778937/135720719-19215224-64fd-4294-af9b-61fdfc52bf05.png)


