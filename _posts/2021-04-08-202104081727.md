---
layout: post
title: "THE	LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---



Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." *arXiv preprint arXiv:1803.03635* (2018).



(기계학습 과제용)

#### 참고 블로그

- <https://towardsdatascience.com/the-lottery-ticket-hypothesis-a-survey-d1f0f62f8884>
- <https://hoya012.github.io/blog/ICLR-2019-best-paper-review/>
- <https://ysbsb.github.io/pruning/2020/04/21/Lottery-ticket-hypothesis.html>
- <https://seing.tistory.com/47>
- <https://velog.io/@bismute/The-Lottery-Ticket-Hypothesis%EC%99%80-%EA%B7%B8-%ED%9B%84%EC%86%8D-%EC%97%B0%EA%B5%AC%EB%93%A4-%EB%A6%AC%EB%B7%B0#%EC%99%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EB%B3%B4%EC%A7%80%EB%8F%84-%EC%95%8A%EA%B3%A0-%EB%94%A5%EB%89%B4%EB%9F%B4%EB%84%B7%EC%97%90%EC%84%9C-%EC%A4%91%EC%9A%94%ED%95%9C-%EB%89%B4%EB%9F%B0%EB%93%A4%EC%9D%84-%EC%B0%BE%EC%95%84%EB%82%BC-%EC%88%98-%EC%9E%88%EB%8A%94%EA%B0%80>



#### Pruning

**Network Pruning**과 관련한 논문.

먼저 pruning이란!

Neural Network가 크면, 그만큼 많은 파라미터가 필요하다. → 메모리 많이 차지. / 계산량 多

따라서 **PRUNING**을 통해 중복되거나 불필요한 부분을 제거한다.

<br>

문제는, Pruning을 통해 찾은 network를 random weight로 initialization시키는 방식(from scratch)을 이용해서 학습을 하면 정확도가 좋지 않았다.

<br>

pruning된 network를 잘 학습시키기 위한 방법 제안!

→ 복권에 당첨된다! (lottery ticket, winning ticket)

<br>

##### 과정

- Train Connectivity : 학습(어떤 연결이 중요한지 알아야 하니까)
- Prune Connections : 불필요한 연결 제거(threshold보다 낮은 것들 싹둑)
- Train Weights : 남아있는 네트워크 학습

<br>

#### The Lottery Ticket Hypothesis

A randomly-initializaed, dense neural network contains a subnetwork that is initialized such that -when trained in isolation- it can match the test accuracy of the original network after training for at most the same number of iterations.

<br>

**<<큰 네트워크에 포함된 하위 네트워크에 winning ticket이 있다.>>**

<br>

원하는 바!!

pruning한 네트워크를 학습해서 original 네트워크보다 test 성능이 더 높은 하위 네트워크를 얻고자 한다. (적은 수의 parameter로 학습)

*기존의 방법으로는 pruning한 네트워크로는 좋은 성능이 나오지 않았는데, parameter가 적어지면서 학습이 잘 되지 않는다는 것이 이유였다.*

<br>

즉. 남아있는 네트워크를 학습시킬 때, weight를 random initialization하면 성능이 좋지 않다는 문제점이 발생.

<br>

<br>

그래서 deep neural network보다 성능이 좋으면서, 학습이 잘 되는  subnetwork(lottery ticket)을 가지고 있다는 가설을 세우고 연구를 진행하였다.

<br>

##### Method of Indentifying winning tickets.

1. Randomly initialize a neural network $$f(x;\theta_{0})$$
2. Train the network for $j$ iterations, arriving at parameters $$\theta_{j}$$
3. Prune $p$% of the parameters in $$\theta_{j}$$, creating a mask m
4. Reset the remaingin parameters to their values in $$\theta_{0}$$, creating the winning ticket $$f(x;m⊙\theta_{0})$$

<br>

4번의 방법만 추가된 것! (3번까지는 기존의 방법들과 동일)

제일 중요한 것: $theta_{0}$

<br>

<br>

한 줄 설명!

**sparse network의 초기값으로 처음의 초기값을 넣어서 학습시킨다.**

++

사용한 것 : iterative pruning

[실험]

- fully-connected networks(Lenet) 
  - MNIST
  - layer-wise pruning을 적용
  - 각 layer마다 weight의 magnitude가 작은 순서대로 pruning을 하는 방식 사용
  - iterative pruning(여러번에 걸쳐서 pruning)
  - reinit(적용하지 않았을 때) 보다 월등한 성능
- Convolutional Networks (ConvNet, Conv랑 Pooling layer로 만듦)
  - CIFAR10
  - layer-wise pruning을 적용
  - heuristic으로 dropout을 같이 사용
  - reinit(적용하지 않았을 때) 보다 월등한 성능
  - early stop iteration → network의 수렴이 빨리 된다는 의미
  - 90% 이상 pruning을 하였을 때에도 pruning을 하기 전과 비슷한 정확도가 유지되는 것을 확인할 수 있음.
  - dropout까지 섞어 쓰면 수렴은 다소 늦게 하지만 test accuracy가 더 높음
- VGG-19 and Resnet-18 (비교적 큰 모델)
  - CIFAR10
  - global pruning
    - 모든 layer에 대하여 한번에 pruning(각 layer 마다 pruning을 하는 대신)
    - 이유 : 네트워크의 layer에 따라 parameter 수의 차이가 크기 때문..각 layer마다 pruning을 적용한다면,, layer의 parameter가 적으면 bottleneck이 되어서 방해된다. 
  - iterative pruning 사용
  - heuristic - learning rate warmup
    - 초기의 learning rate를 초반 iteration 동안 linear하게 증가시키는 방법 의미
    - 다음을 해결하기 위해 사용
      - initial learning rate : 0.1 → network가 학습이 잘 되지만 pruning을 할 시 winning ticket을 찾지 못함
      - initial learning rate : 0.01 → winning ticket은 찾지만 높은 정확도 얻을 수 없음

<br>

<br>

#### DISCUSSION

- 초기화의 중요성
  - 추론해보면, 네트워크의 초기 weight가 최종과 비슷.
  - but, appendix F에서 반대의 경우
  - optimization algorithm, dataset, model과 연관된 것이라고 볼 수 있음.
- winning ticket 구조의 중요성
  - winning ticket을 발견하기 위해 training data를 활용하기 때문에 winning ticket의 구조가 특정 문제에 대해 맞춤화된 inductive bias가 encode되었다고 볼 수 있음.
- neural network 최적화의 의미
  - overparameterize된 network는 winning ticket의 가능성을 가진 더 많은 subnetwork 조합을 포함하고 있기 때문에 더 쉽게 학습 가능 
- heuristic이 많이 개입되어 있음
- Limitation
  - MNIST, CIFAR10과 같은 작은 데이터셋에 대해서만 검증을 한 점
  - 단순히 magnitude에 따라 pruning을 하는 방식.. → library나 hardware 단계에서 속도적인 이점을 얻을 수 없다는 한계
    이를 개선하기 위해 structured pruning과 같은 다른 pruning  technique에도 winning ticket을 적용해 볼 것이라 말함
  - 굉장히 다양한 heuristic이 적용되었음. → 이에 대한 명확한 reasoning이 부족
  - 리뷰
    - ConvNet 구조에 사용한 Batch Normalization으로 인해 winning ticket을 heuristic없이 찾기 힘들지 않을까?
