---
layout: post
title: "FEW-SHOT LEARNING FOR DECODING SURFACE ELECTROMYOGRAPHY FOR HAND GESTURE RECOGNITION"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---

E. Rahimian, S. Zabihi, A. Asif, S. F. Atashzar, and A. Mohammadi, "Few-Shot Learning for Decoding Surface Electromyography for Hand Gesture Recognition," *ICASSP*, 2021, pp. 1300-1304.

----

## Abstract

motivated by the recent advancements of Deep Neural Networks (DNNs)

The recent literature uses traditional supervised learning methods that usually have poor performance if a small amount of data is available or  requires adaptation to a changing task.

Develop **a novel hand gesture recognition framework** based on the formulation of Few-Shot Learning (FSL) <u>to infer the required output given only one or a few numbers of training examples</u>.

Proposed a  new architecture (named as **FHGR** which refers to "Few-shot Hand Gesture Recognition") that learns the mapping <u>using a small number of data and quickly adapts to a new user/gesture by combining its prior experience</u>.

<br>

<br>

## Introduction

noticeable gap between practical settings and recent academic solutions in myoelectric prosthesis control

- First challenge:  "Variation in the sEMG signals"
  - caused by electrode shifts, limb position changes, and non-stationary nature of sEMG signals varying over days, muscular fatigue, skin sweating, or clinical parameters related to the amputation surgery affecting the possibility of accurate recording
  - probability distributions of sEMG signals obtained from one user could be completely different from those of other users, and this could also be different for the same user over different days of use or condition
  - a trained model based on a particular set of measurements cannot directly be utilized over a long period of time
- Second challenge: "Requiring Large and Labelled Datasets"
  - training data가 large할 때는 perform very well 이지만, limit이면 잘 동작하지 않는다.

<br>

### Literature Review

To bridge the gap between usage in real-world and scientific developments under  laboratory conditions, some literature take steps towards **domain adaptation approaches**.



#### TL-based algorithms

→ **to leverage the information and knowledge** gained from multiple users and speed up the learning process for **new users**



#### Domain adaption techniques

→ to introduce robustness against the day-to-day variant nature of the sEMG signals

<br>

### Contributions

1. The FHGR is designed by combining temporal convolutions and attention mechanisms, to reduce the required training time and to mitigate the challenge of variability.
2. The FHGR eliminates the need for extensive recalibration via FSL techniques in which knowledge will be acquired quickly from a few basic exercises.
3. The sEMG adapts, relying on a small number of new observations of new sEMG signal.
4. The FHGR accelerates the learning process for a new user/gesture by utilizing knowledge gained from previous source users/gestures.

<br>

<br>

## The Proposed FHGR Architecture

We extend the FSL to sEMG time-series classification and propose the FHGR framework.

학습 프로세스가 처음부터 매번 학습 x → 적응시간 단축

<br>

#### Preprocessing Step

two inputs

- raw time-domain sEMG signals
- time-channel-frequency spectrograms

Minimax normalization

256-point Fast Fourier Transform (FFT)

<br>

#### Meta-Learning Structure of the FHGR

meta-learning

→ the goal is <u>generalizing the model to the new tasks</u> having seen only <u>a few numbers of updates</u>.

to focus on FSL formulation where **the goal is reducing the prediction error on unlabelled examples given a small training set**.

The FSL model aims to predict the label of the final example based on the previous labels that it has seen.

<br>

#### The FHGR Architecture

The FHGR framework adopts Temporal Convolutions (TC) coupled with attention mechanisms.

- using Dilated Causal 1D-Convolutions
  → the model provides several benefits over RNNs such as easier training, simpler and clearer architecture
- using larger dilation at the top level instead of increasing the number of layers
  → the output can access a broad range of past experiences 
- using attention mechanisms
  → allows the model to identify particular information within a large context

> ☆ **Attention**
>
> 어텐션의 기본 아이디어는 디코더에서 **출력 단어를 예측하는 매 시점(time-step)마다**, 인코더에서의 **전체 입력 문장을 다시 한번 참고**한다는 점
>
> 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, **해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중**해서 보게된다.

<br>

> ☆ **N-way k-shot**
>
> N is the number of class samples from  the set of labels
>
> k denotes the number of examples per each class

<br>

4 modules

- The Embedding Module
  - repeats the following block several times {3 x 3 conv, batch norm, ReLU, Max Pool 2D}
  - adopt three Embedding Modules
    - Spect-Embed1
    - Raw-Embed
    - Spect-Embed2
- TBlock Module
  - consists of two **Dilated Causal 1D-convolutions** (DC-conv)
  - after each DC conv, a **ReLU activation** function is included 
    - to better learn the complex structure of the underlying data
- TCNet Module
  - consists of $log_2_{N x K + 1}$ number of TBlock Module
- The Attention Module
  - An attention block performs key-query similarity measurement to compute weighted average of the values.

<br>

<br>

## Experiments and Results

### Database

the second **Ninapro dataset** referred to as the DB2

→ consists of signals collected from 40 intact-limb  users (28males and 12 females with age 29.9 +- 3.9 years among which 34 are right-handed and 6 are left-handed)

<br>

### Results and Discussions

adam optimizer was used

#### Experiment1 - Classification on New-Repetitions with Few-Shot Observations

<br>

#### Experiment2 - Classification on New-Subject with Few-Shot Observations

the proposed network is trained once and shared across all participants

(this is in contrary to some existing works that train the model separately for each participant)

FHGR achieved acceptable results in transferring information between a source and a target domain despite the existence of a distribution mismatch between them.

<br>

#### Experiment3 - Classification on New-Gestures with Few-Shot Observations

It can be observed that the model can predict unknown class distributions in scenarios where few examples from the target distributions available.

<br>

#### Effects of Different Embedding Modules

raw sEMG signal이 input으로 들어갔을 때, 더 좋은 성능을  보였다.  → it can be concluded that the network itself can extract more informative features, improving the overall performance.



Experiment1에서, Spect-Embed1이 더 좋은 성능을 보였다. → <u>deeper structure</u> of Spect-Embed1 helps it to mimic data distribution better than Spect-Embed2.



대조적으로, Experiment2와 3에서, Spect-Embed1의 성능이 2보다 좋지 않았다. → data distribution in meta_train and meta_test is not the same anymore.

Therefore, <u>the simpler structure in Spect-Embed2</u> contributes to its generalization resulting in better performance on the test-set having a different distribution from that of the train-set.
