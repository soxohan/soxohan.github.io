---
layout: post
title: "Cross-Domain Few-Shot Learning with Meta Fine-Tuning"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---

J. Cai, and S. M. Shen, "Cross-domain few-shot learning with meta fine-tuning," arXiv, 2020

<br>

CD-FSL challenge 2위 논문 → 73.78%

*이것도 마찬가지로 이해안되는 논문 ......  다시 와서 보면 알 수 있을까? ㅜㅜ*

## Abstract

- we build upon state-of-the-art methods in domain adaptation and few-shot learning to create a system that can be trained to perform both tasks.
- we explore the integration of transfer-learning (fine-tuning) with meta learning algorithms, to train a network that has specific layers that are designed to be adapted at a later fine-tuning stage.
  - to train a network that has specific layers that are designed to be adapted at a later fine-tuning stage
- we modify the episodic training process to include a first-order MAML-based meta-learning algorithm, and use a Graph Neural Network model as the subsequent meta-learning module to compare the feature vectors.

<br>

<br>

## Introduction

### contributions

- The integration of fine-tuning into the episodic training process by exploiting a first-order MAML-based meta-learning algorithm (henceforth “Meta Fine-Tuning”). This is done so that the network learns a set of initial weights that be easily fine-tuned on the support-set of the test domain. 
- Second, this paper integrates the above Meta FineTuning algorithm into a Graph Neural Network that exploits the non-Euclidean structure of the relation between the support set and the query samples. 
- Third, as the baseline code-base only implements data augmentation for the training process, we implement data augmentation on the support set during fine-tuning, and achieve a further improvement in accuracy. 
- Finally, we combine the above method with a modified fine-tuning baseline method, and combine them into an ensemble to jointly make predictions.

<br>

<br>

## Relevant work

- The key method that this paper will build on is Graph Neural Networks
- A key method for domain adaptation is to fix earlier feature layers, and fine-tune later feature layers on the support examples

<br>

<br>

## Methodology

### Graph Neural Networks

### Meta Fine-Tuning

The core idea of meta-fine tuning is that instead of finetuning a pre-trained model that was not trained explicitly for fine-tuning, we can use meta-learning to find a set of weight initializations that are intended to be fine-tuned. To this end, we apply and adapt the first-order MAML algorithm [7] and simulate the episodic training process. A first-order MAML algorithm can achieve comparable results with the second order algorithm at a lower computational cost.

<br>

![image](https://user-images.githubusercontent.com/85778937/126874887-1c8b23c8-a5cb-4abe-bc95-c3adb8ee682e.png)

![image](https://user-images.githubusercontent.com/85778937/126874895-10d8d4d1-c031-41ed-a18b-0ae4322662d8.png)

### Data Augmentation

### Combining Scores in the Ensemble

### Memory Requirements of GNN on 50-shots

The GNN builds a densely-connected graph between all the support samples and each new query sample.
