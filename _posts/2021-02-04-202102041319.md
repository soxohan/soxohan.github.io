---
layout: post
title: "RNN+BPTT"
name : "study about..RNN"
description : "SOSOHAN study"
category: study
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
use_math : true
---

# RNN+BPTT

```
계속 추가 예정..
```



## RNN

피드포워드(흐름이 단방향) 구조의 단점 중 하나인 시계열 데이터!!



즉, 시계열 데이터의 패턴을 학습할 수 없어서 **순환신경망 Recurrent Neural Network**의 등장.



따라서 <u>순환하는 경로</u>가 존재한다. 이 순환 경로를 따라 과거의 정보를 기억하면서 최신 정보의 갱신도 가능. 


$h_t = tanh(h_{t-1}W_h + X_tW_x + b)$

즉 상태를 기억해서 1 step 진행될 때마다 위의 식을 이용하여 갱신. h를 은닉 상태(Hidden State) / 은닉 상태 벡터(Hidden State Vector)



## BPTT

Backpropagation Through Time을 이용하여 RNN 학습 가능.

하지만 시계열 데이터의 크기가 커지는 것과 비례하여 컴퓨팅 시간도 증가한다. 또한 컴퓨팅 시간이 증가하면 vanishing gradient problem도 발생한다.



이러한 문제를 해결하기 위하여 Truncated BPTT가 나왔다. 길어진 신경망을 잘라서 여러 블록으로 만든다. 여기서 자르는 것은 역전파의 연결만을 자른다. 잘라낸 블록 단위로 오차역전파법을 수행. 
