---
layout: post
title: "THE	LOTTERY TICKET HYPOTHESIS: FINDING SPARSE, TRAINABLE NEURAL NETWORKS"
category: paper
changefreq : daily
priority : 1.0
comments : true
permalink : /:categories/:year/:month/:day/:title/
math_use : true
sitemap : false
---



Frankle, Jonathan, and Michael Carbin. "The lottery ticket hypothesis: Finding sparse, trainable neural networks." *arXiv preprint arXiv:1803.03635* (2018).



(과제용)



#### Pruning

Network Pruning과 관련한 논문.

먼저 pruning이란!

Neural Network가 크면, 그만큼 많은 파라미터가 필요하다. → 메모리 많이 차지. / 계산량 多

따라서 **PRUNING**을 통해 중복되거나 불필요한 부분을 제거한다.

<br>

##### 과정

- Train Connectivity : 학습(어떤 연결이 중요한지 알아야 하니까)
- Prune Connections : 불필요한 연결 제거(threshold보다 낮은 것들 싹둑)
- Train Weights : 남아있는 네트워크 학습

<br>

**But**, 남아있는 네트워크를 학습시킬 때, weight를 random initialization하면 성능이 좋지 않다는 문제점이 발생.



#### The Lottery Ticket Hypothesis

A randomly-initializaed, dense neural network contains a subnetwork that is initialized such that -when trained in isolation- it can match the test accuracy of the original network after training for at most the same number of iterations.

<br>

<<큰 네트워크에 포함된 하위 네트워크에 winning ticket이 있다.>>

<br>

원하는 바!!

pruning한 네트워크를 학습해서 original 네트워크보다 test 성능이 더 높은 하위 네트워크를 얻고자 한다. (적은 수의 parameter로 학습)

*기존의 방법으로는 pruning한 네트워크로는 좋은 성능이 나오지 않았는데, parameter가 적어지면서 학습이 잘 되지 않는다는 것이 이유였다.*

<br>

<br>

##### Method of Indentifying winning tickets.

1. Randomly initialize a neural network $f(x;\theta_{0})$
2. Train the network for $j$ iterations, arriving at parameters $\theta_{j}$
3. Prune $p$% of the parameters in $\theta_{j}$, creating a mask m
4. Reset the remaingin parameters to their values in $\theta_{0}$, creating the winning ticket $f(x;m⊙\theta_{0})$



<br>

한 줄 설명!

**sparse network의 초기값으로 처음의 초기값을 넣어서 학습시킨다.**

++

사용한 것 : iterative pruning



- fully-connected networks(Lenet) 
  - MNIST
- Convolutional Networks
  - CIFAR10
- VGG and Resnet 
  - CIFAR10
  - global pruning
    - 모든 layer에 대하여 한번에 pruning
    - 이유 : 네트워크의 layer에 따라 parameter 수의 차이가 크기 때문.. layer의 parameter가 적으면 bottleneck이 되어서 방해된다. 



#### DISCUSSION

- 초기화의 중요성
  - 추론해보면, 네트워크의 초기 weight가 최종과 비슷.
  - but, appendix F에서 반대의 경우
  - optimization algorithm, dataset, model과 연관된 것이라고 볼 수 있음.
- 